---
title: "Batch effects_CpG"
author: "Katya Murzin"
date: "2023-08-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Load packages
```{r}
library(tictoc)
library(tidyverse)
library(cyCombine)
library(emdist)
library(readr)
library(sva)
#library(arrow)
tic(msg = "start compute time")
```

## Notes------------------------------------------------------------------------
chunk4-CpG_Live_Cts object has NA values at the following positions. 
```{R findNA, include = TRUE}
which(is.na(CpG_Live_Cts), arr.ind=TRUE)
``` 
Do not know if this impacts analyses or is a known issue. 

Removed NAs and updated conditions groups 
  -CpG/NoBurn = Control 
  -GpC/NoBurn = Sham
  
Chunk5- dataset is quite large (4.2GBs), long load time on local machine. 
Consider modulating as an arrow object. Need to consider how this impacts data 
format, if at all. 

##-To-Do/Edits-to-make----------------------------------------------------------
Modify outpaths for saving denspplots and Umaps. 

Discuss and potentially modify the condition groups for metadata. 

Reference AMP batch correction script to determine the correct covariates to 
correct for. 
  -Covariate for Amp is Disease not any treatment. Seems comparable to run 
  Condition as covariate to crrect for in this dataset. 

Run benchmarking on the corrected datasets

--------------------------------------------------------------------------------

```{r}
CpG_Live_Cts <- read_csv("CpG_counts.csv",
                       lazy = F)
CpG_Live_Cts$Batch <- factor(CpG_Live_Cts$Batch, levels = unique(CpG_Live_Cts$Batch))
colnames(CpG_Live_Cts) <- make.names(colnames(CpG_Live_Cts))
CpG_Live_MD <- CpG_Live_Cts %>% select(file, Batch, Condition, Treatment, Type)

```
checked 4/20
```{r}
CpG_Live_Cells <- read_csv("CpG_all.csv",
                        lazy = F)
marker_names_pre <- colnames(CpG_Live_Cells)
colnames(CpG_Live_Cells)[8:78] <- sapply(colnames(CpG_Live_Cells)[8:78], function(x){strsplit(x, "___")[[1]][2]})
colnames(CpG_Live_Cells)[79] <- "file"
colnames(CpG_Live_Cells) <- make.names(colnames(CpG_Live_Cells))
markers <- colnames(CpG_Live_Cells)[c(9,18:23,33:68, 73:76, 78)]
CpG_Live_Cells <- merge(CpG_Live_MD, CpG_Live_Cells, by = "file", all.y = T)
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Batch))]
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Condition))]
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Treatment))]
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Type))]
```

##Import Data With Arrow--------------------------------------------------------
```{r importwitharrow}
CpG_Live_Cells <- read_csv_arrow(here::here("CpG_all.csv"))
toc()
```
Even import entire dataset with arrow function is considerably faster. 
## Import without Arrow---------------------------------------------------------

```{r}
tic(msg = "Start Import CSV")

CpG_Live_Cells <-read.csv(here::here("CpG_all.csv"))

toc()
```
##Note Show in New Window
Start Import CSV: 1022.426 sec elapsed



```{r}
colnames(CpG_Live_Cells)[8:78] <- sub("___", "/", colnames(CpG_Live_Cells[8:78]))
colnames(CpG_Live_Cells)[8:78] <-gsub("\\/.*", "", colnames(CpG_Live_Cells[8:78]))
```



```{r cleanupdata1, include=FALSE}
marker_names_pre <- colnames(CpG_Live_Cells)
#colnames(CpG_Live_Cells)[8:78] <- sapply(colnames(CpG_Live_Cells)[8:78], function(x){strsplit(x, "___")[[1]][2]})
colnames(CpG_Live_Cells)[79] <- "file"
colnames(CpG_Live_Cells) <- make.names(colnames(CpG_Live_Cells))
markers <- colnames(CpG_Live_Cells)[c(9,18:23,33:68, 73:76, 78)]
CpG_Live_Cells <- merge(CpG_Live_MD, CpG_Live_Cells, by = "file", all.y = T)
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Batch))]
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Condition))]
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Treatment))]
CpG_Live_Cells$file[which(is.na(CpG_Live_Cells$Type))]
```

condition is the only MD with NA. 
Updated groups missing medata values 
  -CpG/NoBurn = Control 
  -GpC/NoBurn = Sham 

```{r}
CpG_Live_Cells |> 
  head()
```

```{r conditionQC, include = TRUE}
CpG_Live_Cts |>
  group_by(Condition) |>
  summarise(
    patient_group_counts = n()
  ) |>
  collect()
```

Need to double check the number of samples from each group. Sham Has 19, which 
makes sense because we lost one mouse to infection before harvest. 

--------------------------------------------------------------------------------


checked 4/20
```{r}
CpG_Live_Cells[, c(14,23:28,38:73,78:81,83)] <- transform_asinh(CpG_Live_Cells[, c(14, 23:28,38:73,78:81,83)])
```
 
checked 4/20
```{r}
tic(msg = "Detect Batch Effects")
colnames(CpG_Live_Cells)[2] <- "batch"
colnames(CpG_Live_Cells)[1] <- "sample"
detect_batch_effect(CpG_Live_Cells[c(1, 2, 14, 23:28,38:73,78:81,83)], out_dir = getwd(), markers = markers)
toc()
```

##Note----9/29/2023-------------------------------------------------------------
Detect Batch effects function works. Total elapsed time = 713.63 secs (11 min)

Results from 9.29
There are 0 markers that appear to be outliers in a single batch:

There are 0 clusters, in which a single cluster is strongly over- or 
underrepresented.
Making UMAP plots for up to 50,000 cells.



##Note----9/23/23---------------------------------------------------------------
error while running: 
  -vectore memory exhausted (limit reached?)
  Have not run into this before. Will have to run analysis on RStudio Pro Server
Copy github repsoitory and load in data. 
  -Could potentially save as a parquet file and load into github. 
  -again may effect data structure. 

--------------------------------------------------------------------------------


```{r}
tic(msg = "Correct Batch Effects")
corrected <- batch_correct(CpG_Live_Cells, covar = CpG_Live_Cells$Condition, markers = markers)
toc()
```
##Note-----9/27/2023------------------------------------------------------------
correct batch effects function works. 
Time elapsed: 553.278 sec elapsed


Appears to be very little batch effects to correct for. Need to make sure not 
due to lack of controls or inaccurate conditions. 

--------------------------------------------------------------------------------

```{r}
tic(msg = "Densplt start")
densplt <- plot_density(CpG_Live_Cells, corrected, ncol = 5, markers = markers, filename = here::here("Densplot_9_26_23.pdf"))
toc()
```

```{r}
labels <- corrected %>% 
  create_som(markers = markers,
             rlen = 10)
CpG_Live_Cells$label <- corrected$label <- labels
CpG_Live_Cells$id <- corrected$id
emd <- evaluate_emd(uncorrected = CpG_Live_Cells, corrected = corrected, markers = markers, cell_col = "label")
```

```{r}
mad <- evaluate_mad(uncorrected = CpG_Live_Cells, corrected = corrected, markers = markers, cell_col = "label")
```
##MAD results-------9/27/2023---------------------------------------------------
The MAD score is: 0.01

--------------------------------------------------------------------------------


```{r, include = TRUE}
emdviol <- emd$violin

emdscatter <- emd$scatter

emdred <- emd$reduction

emddat <- emd$emd
```

```{r}
unscale <- function(x){
  sinh(x)*5
}
unscaled <- corrected
unscaled[,c(3:50)] <- sapply(corrected[,c(3:50)], unscale)
```


## Note---9/27/2023-------------------------------------------------------------
chunk below will cause files to be written by batch. If want files written by 
sample then change unscaled$batch to unscaled$sample

--------------------------------------------------------------------------------
```{r}
final <- split(unscaled, unscaled$batch)
```

```{r}
for(i in 1:length(final)){
  tbl <- final[[i]]
  path <- paste0(here::here(),"/Corrected_R1_9_27/", names(final[i]), ".csv")
  write_csv(tbl[,c(3:85)], file = path)
}
```

